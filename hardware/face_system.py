import cv2
import face_recognition
import sqlite3
import json
import numpy as np
import time
import warnings
import os

# å±è”½æ— å…³ç´§è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="face_recognition_models")

# ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä½æ•°æ®åº“ (å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ä¸Šä¸€çº§ -> æ ¹ç›®å½•)
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
DATABASE_NAME = os.path.join(PROJECT_ROOT, "capsule_dispenser.db")

class FaceRecognizer:
    def __init__(self):
        self.known_face_encodings = []
        self.known_face_ids = []
        self.cap = None
        self.last_scan_time = 0
        self.scan_interval = 0.5  # é™åˆ¶è¯†åˆ«é¢‘ç‡ï¼Œæ¯ 0.5 ç§’ä¸€æ¬¡ï¼Œé˜²æ­¢ CPU æ»¡è½½
        
        # 1. åŠ è½½å·²çŸ¥äººè„¸
        self.load_faces_from_db()
        
        # 2. åˆå§‹åŒ–æ‘„åƒå¤´
        self.init_camera()

    def load_faces_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰å·²å½•å…¥çš„äººè„¸ç‰¹å¾"""
        print("ğŸ‘¤ [Face] æ­£åœ¨åŠ è½½äººè„¸æ•°æ®åº“...")
        try:
            conn = sqlite3.connect(DATABASE_NAME)
            cursor = conn.cursor()
            # åªåŠ è½½æœ‰äººè„¸æ•°æ®çš„ç”¨æˆ·
            cursor.execute("SELECT user_id, name, face_encoding FROM Users WHERE face_encoding IS NOT NULL")
            rows = cursor.fetchall()
            
            self.known_face_encodings = []
            self.known_face_ids = []
            count = 0
            
            for uid, name, encoding_json in rows:
                if encoding_json:
                    try:
                        # JSON -> List -> Numpy Array
                        encoding_list = json.loads(encoding_json)
                        encoding_np = np.array(encoding_list)
                        self.known_face_encodings.append(encoding_np)
                        self.known_face_ids.append(uid)
                        count += 1
                    except Exception as e:
                        print(f"  âš ï¸ ç”¨æˆ· {name} (ID {uid}) æ•°æ®æŸå: {e}")
            
            conn.close()
            print(f"ğŸ‘¤ [Face] å·²åŠ è½½ {count} ä¸ªç”¨æˆ·çš„äººè„¸æ•°æ®")
        except Exception as e:
            print(f"âŒ [Face] æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")

    def init_camera(self):
        """ä½¿ç”¨ Pi 5 å…¼å®¹ç­–ç•¥åˆå§‹åŒ–æ‘„åƒå¤´"""
        print("ğŸ“· [Face] åˆå§‹åŒ–æ‘„åƒå¤´...")
        
        # 1. å°è¯• GStreamer ç­–ç•¥
        gst_pipelines = [
            (
                "libcamerasrc ! video/x-raw,format=NV12,width=640,height=480,framerate=30/1 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1",
                "GStreamer (NV12)"
            ),
            (
                "libcamerasrc ! video/x-raw,width=640,height=480 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1",
                "GStreamer (Auto)"
            )
        ]

        for pipeline, name in gst_pipelines:
            try:
                # print(f"  -> å°è¯• {name}...")
                cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret and frame is not None and frame.size > 0:
                        print(f"âœ… [Face] æ‘„åƒå¤´å°±ç»ª: {name}")
                        self.cap = cap
                        return
                    else:
                        cap.release()
            except Exception:
                pass

        # 2. å¦‚æœ GStreamer å¤±è´¥ï¼Œéå†æœç´¢ V4L2 è®¾å¤‡ (0-20)
        print("âš ï¸ [Face] GStreamer å¤±è´¥ï¼Œæ­£åœ¨æœç´¢ V4L2 è®¾å¤‡...")
        for i in range(20):
            try:
                cap = cv2.VideoCapture(i, cv2.CAP_V4L2)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret and frame is not None and frame.size > 0:
                        print(f"âœ… [Face] æˆåŠŸè¿æ¥ V4L2 è®¾å¤‡ (Index: {i})")
                        self.cap = cap
                        
                        # ä¿å­˜ä¸€å¼ è°ƒè¯•å›¾ï¼Œç¡®ä¿ç”»é¢æ­£å¸¸
                        cv2.imwrite("debug_camera_view.jpg", frame)
                        print(f"   [Debug] å·²ä¿å­˜æµ‹è¯•å›¾åˆ° debug_camera_view.jpg")
                        return
                    else:
                        cap.release()
            except:
                pass
        
        print("âŒ [Face] æ— æ³•åˆå§‹åŒ–ä»»ä½•æ‘„åƒå¤´ï¼Œäººè„¸è¯†åˆ«å°†ä¸å¯ç”¨")
        self.cap = None

    def scan(self):
        """
        å°è¯•è¯»å–ä¸€å¸§å¹¶è¯†åˆ«ã€‚
        è¿”å›: matched_user_id (int) æˆ– None
        """
        # å¦‚æœæ²¡æ‘„åƒå¤´æˆ–æ²¡äººè„¸åº“ï¼Œç›´æ¥è·³è¿‡
        if not self.cap or not self.known_face_encodings:
            return None

        # é¢‘ç‡æ§åˆ¶
        if time.time() - self.last_scan_time < self.scan_interval:
            return None
        self.last_scan_time = time.time()

        ret, frame = self.cap.read()
        if not ret:
            print("âš ï¸ [Face] æ— æ³•è¯»å–è§†é¢‘å¸§ (Stream broken)")
            # å°è¯•é‡è¿é€»è¾‘å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            return None

        # ä¼˜åŒ–ç­–ç•¥: Pi 5 æ€§èƒ½è¶³å¤Ÿï¼Œä¸å†ç¼©å°å›¾åƒï¼Œä»¥æé«˜æš—å…‰/è¿œè·ç¦»æ£€æµ‹ç‡
        # small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5) 
        
        # 1. è½¬æ¢ä¸ºç°åº¦å›¾ç”¨äºå¢å¼º (Detection éœ€è¦)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # 2. å›¾åƒå¢å¼º: CLAHE (é™åˆ¶å¯¹æ¯”åº¦è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–)
        # è¿™èƒ½æ˜¾è‘—æ”¹å–„æš—å…‰ä¸‹çš„äººè„¸å¯è§åº¦
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        enhanced_gray = clahe.apply(gray)
        
        # å°†å¢å¼ºåçš„ç°åº¦å›¾è½¬å› RGB (face_recognition éœ€è¦ RGBï¼Œä½†å…¶å®å®ƒå†…éƒ¨ä¹Ÿä¼šè½¬ç°åº¦ï¼Œ
        # ä¸è¿‡æˆ‘ä»¬ç”¨å¢å¼ºè¿‡çš„é€šé“æ›¿æ¢åŸå§‹äº®åº¦ï¼Œèƒ½è¾…åŠ©æ£€æµ‹)
        # è¿™é‡Œä¸ºäº†ç®€å•å…¼å®¹ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨åŸå›¾è½¬ RGB ç”¨äºç‰¹å¾æå–ï¼Œ
        # ä½†ç”¨å¢å¼ºå›¾åšæ£€æµ‹å¯èƒ½ä¼šæ›´å¤æ‚ (åº“æ¥å£é™åˆ¶)ã€‚
        # 
        # ä¿®æ­£æ–¹æ¡ˆ: face_recognition åº“ä¸»è¦ä¾èµ– HOGã€‚
        # æˆ‘ä»¬ç›´æ¥æŠŠåŸå›¾è½¬ RGBï¼Œä¸ç¼©å°ã€‚
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # 3. æ£€æµ‹äººè„¸ (ä½¿ç”¨åŸåˆ†è¾¨ç‡ 640x480)
        face_locations = face_recognition.face_locations(rgb_frame)
        if not face_locations:
            # æ²¡äººè„¸æ—¶ä¿æŒé™é»˜
            return None 

        # 4. æå–ç‰¹å¾
        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        
        print(f"ğŸ‘€ [Face] æ•è·åˆ° {len(face_encodings)} å¼ äººè„¸")

        # 4. æ¯”å¯¹
        for face_encoding in face_encodings:
            # è®¡ç®—ä¸æ•°æ®åº“ä¸­æ‰€æœ‰äººè„¸çš„æ¬§æ°è·ç¦»
            # è·ç¦»è¶Šå°è¶Šç›¸ä¼¼ã€‚é€šå¸¸ 0.6 æ˜¯åˆ†ç•Œçº¿ã€‚
            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)
            
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„é‚£ä¸ª
            best_match_index = np.argmin(face_distances)
            min_distance = face_distances[best_match_index]

            # é˜ˆå€¼è°ƒæ•´è¯´æ˜:
            # 0.60: æ ‡å‡†ä¸¥æ ¼é˜ˆå€¼ (è¯¯è¯†ç‡æä½ï¼Œä½†æ‹’è¯†ç‡é«˜)
            # 0.72: å®½æ¾é˜ˆå€¼ (é€‚åˆæ ‘è“æ´¾æ‘„åƒå¤´åŠéå—æ§å…‰çº¿ï¼Œä½“éªŒæ›´å¥½)
            if min_distance < 0.72: 
                user_id = self.known_face_ids[best_match_index]
                print(f"ğŸ‘¤ [Face] è¯†åˆ«æˆåŠŸ! ID: {user_id} (è·ç¦»: {min_distance:.2f})")
                return user_id
            else:
                print(f"ğŸ¤” [Face] é™Œç”Ÿäºº (æœ€è¿‘è·ç¦»: {min_distance:.2f})")
        
        return None

    def close(self):
        if self.cap:
            self.cap.release()
