import cv2
import face_recognition
import sqlite3
import json
import numpy as np
import time
import warnings
import os

# å±è”½æ— å…³ç´§è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="face_recognition_models")

# ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä½æ•°æ®åº“ (å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ä¸Šä¸€çº§ -> æ ¹ç›®å½•)
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
DATABASE_NAME = os.path.join(PROJECT_ROOT, "capsule_dispenser.db")

class FaceRecognizer:
    def __init__(self):
        self.known_face_encodings = []
        self.known_face_ids = []
        self.cap = None
        self.last_scan_time = 0
        self.scan_interval = 0.5  # é™åˆ¶è¯†åˆ«é¢‘ç‡
        self.no_face_count = 0    # è°ƒè¯•è®¡æ•°å™¨
        
        # 1. åŠ è½½å·²çŸ¥äººè„¸
        self.load_faces_from_db()
        
        # 2. åˆå§‹åŒ–æ‘„åƒå¤´
        self.init_camera()

    def load_faces_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰å·²å½•å…¥çš„äººè„¸ç‰¹å¾"""
        print("ğŸ‘¤ [Face] æ­£åœ¨åŠ è½½äººè„¸æ•°æ®åº“...")
        try:
            conn = sqlite3.connect(DATABASE_NAME)
            cursor = conn.cursor()
            # åªåŠ è½½æœ‰äººè„¸æ•°æ®çš„ç”¨æˆ·
            cursor.execute("SELECT user_id, name, face_encoding FROM Users WHERE face_encoding IS NOT NULL")
            rows = cursor.fetchall()
            
            self.known_face_encodings = []
            self.known_face_ids = []
            count = 0
            
            for uid, name, encoding_json in rows:
                if encoding_json:
                    try:
                        # JSON -> List -> Numpy Array
                        encoding_list = json.loads(encoding_json)
                        encoding_np = np.array(encoding_list)
                        self.known_face_encodings.append(encoding_np)
                        self.known_face_ids.append(uid)
                        count += 1
                    except Exception as e:
                        print(f"  âš ï¸ ç”¨æˆ· {name} (ID {uid}) æ•°æ®æŸå: {e}")
            
            conn.close()
            print(f"ğŸ‘¤ [Face] å·²åŠ è½½ {count} ä¸ªç”¨æˆ·çš„äººè„¸æ•°æ®")
        except Exception as e:
            print(f"âŒ [Face] æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")

    def init_camera(self):
        """ä½¿ç”¨ Pi 5 å…¼å®¹ç­–ç•¥åˆå§‹åŒ–æ‘„åƒå¤´"""
        print("ğŸ“· [Face] åˆå§‹åŒ–æ‘„åƒå¤´...")
        
        # 1. å°è¯• GStreamer ç­–ç•¥
        gst_pipelines = [
            (
                "libcamerasrc ! video/x-raw,format=NV12,width=640,height=480,framerate=30/1 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1",
                "GStreamer (NV12)"
            ),
            (
                "libcamerasrc ! video/x-raw,width=640,height=480 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1",
                "GStreamer (Auto)"
            )
        ]

        for pipeline, name in gst_pipelines:
            try:
                # print(f"  -> å°è¯• {name}...")
                cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret and frame is not None and frame.size > 0:
                        print(f"âœ… [Face] æ‘„åƒå¤´å°±ç»ª: {name}")
                        self.cap = cap
                        return
                    else:
                        cap.release()
            except Exception:
                pass

        print("âŒ [Face] æ— æ³•åˆå§‹åŒ– GStreamer æ‘„åƒå¤´ï¼Œäººè„¸è¯†åˆ«å°†ä¸å¯ç”¨")
        print("   æç¤º: è¯·æ£€æŸ¥æ‘„åƒå¤´æ’çº¿æ˜¯å¦æ’å¥½ï¼Œä»¥åŠæ˜¯å¦å®‰è£…äº† gstreamer1.0-libcamera")
        self.cap = None

    def scan(self):
        """
        æ ¸å¿ƒå‡½æ•°ï¼šå°è¯•è¯»å–ä¸€å¸§å¹¶è¯†åˆ«ã€‚
        è¿”å›: matched_user_id (int) æˆ– None
        """
        # å¦‚æœæ²¡æ‘„åƒå¤´æˆ–æ²¡äººè„¸åº“ï¼Œç›´æ¥è·³è¿‡ï¼Œä¸åšæ— ç”¨åŠŸ
        if not self.cap or not self.known_face_encodings:
            return None

        # é¢‘ç‡æ§åˆ¶ (Throttling)
        # é˜²æ­¢è·‘å¾—å¤ªå¿«å æ»¡ CPUï¼Œæ¯ç§’åªæ‰«æ 2 æ¬¡ (1/0.5s)
        if time.time() - self.last_scan_time < self.scan_interval:
            return None
        self.last_scan_time = time.time()

        ret, frame = self.cap.read()
        if not ret:
            print("âš ï¸ [Face] æ— æ³•è¯»å–è§†é¢‘å¸§ (Stream broken)")
            return None

        # --- æ—‹è½¬å›¾åƒ (Rotation) ---
        # é€‚é…ç‰©ç†å®‰è£…ï¼šæ‘„åƒå¤´é€†æ—¶é’ˆæ—‹è½¬äº† 90 åº¦ (Counter-Clockwise)
        frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)

        # --- å›¾åƒå¢å¼º (Image Enhancement) ---
        # æ ‘è“æ´¾æ‘„åƒå¤´åœ¨å®¤å†…å¾€å¾€å…‰çº¿ä¸è¶³ã€‚
        # è¿™é‡Œä½¿ç”¨äº† CLAHE (å¯¹æ¯”åº¦å—é™è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–) ç®—æ³•ã€‚
        # ç®€å•æ¥è¯´ï¼šå®ƒæŠŠç”»é¢åˆ‡æˆå°å—ï¼ŒæŠŠå¤ªæš—çš„åœ°æ–¹æäº®ï¼ŒæŠŠå¤ªäº®çš„åœ°æ–¹å‹æš—ã€‚
        
        # 1. BGR è½¬ LAB: å› ä¸ºåœ¨ RGB æ¨¡å¼ä¸‹è°ƒäº®åº¦ä¼šæ”¹å˜é¢œè‰²ï¼ŒLAB æ¨¡å¼æŠŠäº®åº¦(L)å’Œé¢œè‰²(A,B)åˆ†å¼€äº†ã€‚
        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab) # æ‹†åˆ†é€šé“

        # 2. å¯¹ L (äº®åº¦) é€šé“åº”ç”¨å¢å¼º
        # clipLimit=3.0: é™åˆ¶å¯¹æ¯”åº¦å¢å¼ºçš„å€æ•°ï¼Œé˜²æ­¢å™ªå£°ä¹Ÿè¢«æ”¾å¤§
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        cl = clahe.apply(l)

        # 3. åˆå¹¶é€šé“å¹¶è½¬å› RGB (äººè„¸åº“éœ€è¦ RGB)
        limg = cv2.merge((cl, a, b))
        enhanced_frame = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)

        # --- äººè„¸æ£€æµ‹ (Detection) ---
        # å¯»æ‰¾ç”»é¢ä¸­å“ªé‡Œæœ‰äººè„¸ (è¿”å›åæ ‡æ¡†ï¼štop, right, bottom, left)
        face_locations = face_recognition.face_locations(enhanced_frame)
        
        if not face_locations:
            self.no_face_count += 1
            if self.no_face_count % 20 == 0: 
                pass # è¿ç»­æ²¡æ£€æµ‹åˆ°äººè„¸æ—¶ï¼Œé™é»˜å¤„ç†
            return None 
        
        if self.no_face_count > 0:
            self.no_face_count = 0

        # --- ç‰¹å¾æå– (Encoding) ---
        # æŠŠäººè„¸å›¾åƒè½¬æ¢æˆä¸€ä¸ª 128ç»´ çš„å‘é‡ (ä¸€ç»„æ•°å­—)ã€‚
        # åªè¦æ˜¯åŒä¸€ä¸ªäººï¼Œæ— è®ºè§’åº¦å¦‚ä½•ï¼Œè¿™ä¸ªå‘é‡çš„æ•°å€¼éƒ½å¾ˆæ¥è¿‘ã€‚
        face_encodings = face_recognition.face_encodings(enhanced_frame, face_locations)
        
        print(f"ğŸ‘€ [Face] æ•è·åˆ° {len(face_encodings)} å¼ äººè„¸")

        # --- äººè„¸æ¯”å¯¹ (Matching) ---
        for face_encoding in face_encodings:
            # è®¡ç®—å½“å‰äººè„¸å‘é‡ä¸æ•°æ®åº“ä¸­æ‰€æœ‰å‘é‡çš„æ¬§æ°è·ç¦» (Euclidean Distance)
            # è·ç¦»è¶Šå° = è¶Šç›¸ä¼¼ã€‚
            # è·ç¦» = 0: å®Œå…¨ä¸€æ ·
            # è·ç¦» > 0.6: é€šå¸¸è®¤ä¸ºæ˜¯ä¸åŒçš„äºº
            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)
            
            # æ‰¾åˆ°è·ç¦»æœ€å°çš„é‚£ä¸ª (æœ€åƒçš„é‚£ä¸ª)
            # np.argmin è¿”å›æœ€å°å€¼æ‰€åœ¨çš„ç´¢å¼•ä½ç½®
            best_match_index = np.argmin(face_distances)
            min_distance = face_distances[best_match_index]

            # é˜ˆå€¼åˆ¤å®š
            # 0.60: æ ‡å‡†ä¸¥æ ¼é˜ˆå€¼ (Standard)
            # 0.68: é’ˆå¯¹å½“å‰ç¯å¢ƒè°ƒæ•´ (User Obs: 0.65)
            if min_distance < 0.68: 
                user_id = self.known_face_ids[best_match_index]
                print(f"ğŸ‘¤ [Face] è¯†åˆ«æˆåŠŸ! ID: {user_id} (è·ç¦»: {min_distance:.2f})")
                return user_id
            else:
                print(f"ğŸ¤” [Face] é™Œç”Ÿäºº (æœ€è¿‘è·ç¦»: {min_distance:.2f})")
        
        return None

    def close(self):
        if self.cap:
            self.cap.release()
