import cv2
import face_recognition
import sqlite3
import json
import numpy as np
import time
import warnings
import os

# å±è”½æ— å…³ç´§è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="face_recognition_models")

# ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä½æ•°æ®åº“ (å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ä¸Šä¸€çº§ -> æ ¹ç›®å½•)
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
DATABASE_NAME = os.path.join(PROJECT_ROOT, "capsule_dispenser.db")

class FaceRecognizer:
    def __init__(self):
        self.known_face_encodings = []
        self.known_face_ids = []
        self.cap = None
        self.last_scan_time = 0
        self.scan_interval = 0.5  # é™åˆ¶è¯†åˆ«é¢‘ç‡
        self.no_face_count = 0    # è°ƒè¯•è®¡æ•°å™¨
        
        # 1. åŠ è½½å·²çŸ¥äººè„¸
        self.load_faces_from_db()
        
        # 2. åˆå§‹åŒ–æ‘„åƒå¤´
        self.init_camera()

    def load_faces_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰å·²å½•å…¥çš„äººè„¸ç‰¹å¾"""
        print("ğŸ‘¤ [Face] æ­£åœ¨åŠ è½½äººè„¸æ•°æ®åº“...")
        try:
            conn = sqlite3.connect(DATABASE_NAME)
            cursor = conn.cursor()
            # åªåŠ è½½æœ‰äººè„¸æ•°æ®çš„ç”¨æˆ·
            cursor.execute("SELECT user_id, name, face_encoding FROM Users WHERE face_encoding IS NOT NULL")
            rows = cursor.fetchall()
            
            self.known_face_encodings = []
            self.known_face_ids = []
            count = 0
            
            for uid, name, encoding_json in rows:
                if encoding_json:
                    try:
                        # JSON -> List -> Numpy Array
                        encoding_list = json.loads(encoding_json)
                        encoding_np = np.array(encoding_list)
                        self.known_face_encodings.append(encoding_np)
                        self.known_face_ids.append(uid)
                        count += 1
                    except Exception as e:
                        print(f"  âš ï¸ ç”¨æˆ· {name} (ID {uid}) æ•°æ®æŸå: {e}")
            
            conn.close()
            print(f"ğŸ‘¤ [Face] å·²åŠ è½½ {count} ä¸ªç”¨æˆ·çš„äººè„¸æ•°æ®")
        except Exception as e:
            print(f"âŒ [Face] æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")

    def init_camera(self):
        """ä½¿ç”¨ Pi 5 å…¼å®¹ç­–ç•¥åˆå§‹åŒ–æ‘„åƒå¤´"""
        print("ğŸ“· [Face] åˆå§‹åŒ–æ‘„åƒå¤´...")
        
        # 1. å°è¯• GStreamer ç­–ç•¥
        gst_pipelines = [
            (
                "libcamerasrc ! video/x-raw,format=NV12,width=640,height=480,framerate=30/1 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1",
                "GStreamer (NV12)"
            ),
            (
                "libcamerasrc ! video/x-raw,width=640,height=480 ! videoconvert ! video/x-raw,format=BGR ! appsink drop=1",
                "GStreamer (Auto)"
            )
        ]

        for pipeline, name in gst_pipelines:
            try:
                # print(f"  -> å°è¯• {name}...")
                cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret and frame is not None and frame.size > 0:
                        print(f"âœ… [Face] æ‘„åƒå¤´å°±ç»ª: {name}")
                        self.cap = cap
                        return
                    else:
                        cap.release()
            except Exception:
                pass

        # 2. å¦‚æœ GStreamer å¤±è´¥ï¼Œéå†æœç´¢ V4L2 è®¾å¤‡ (0-20)
        print("âš ï¸ [Face] GStreamer å¤±è´¥ï¼Œæ­£åœ¨æœç´¢ V4L2 è®¾å¤‡...")
        for i in range(20):
            try:
                cap = cv2.VideoCapture(i, cv2.CAP_V4L2)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret and frame is not None and frame.size > 0:
                        print(f"âœ… [Face] æˆåŠŸè¿æ¥ V4L2 è®¾å¤‡ (Index: {i})")
                        self.cap = cap
                        
                        # ä¿å­˜ä¸€å¼ è°ƒè¯•å›¾ï¼Œç¡®ä¿ç”»é¢æ­£å¸¸
                        cv2.imwrite("debug_camera_view.jpg", frame)
                        print(f"   [Debug] å·²ä¿å­˜æµ‹è¯•å›¾åˆ° debug_camera_view.jpg")
                        return
                    else:
                        cap.release()
            except:
                pass
        
        print("âŒ [Face] æ— æ³•åˆå§‹åŒ–ä»»ä½•æ‘„åƒå¤´ï¼Œäººè„¸è¯†åˆ«å°†ä¸å¯ç”¨")
        self.cap = None

    def scan(self):
        """
        å°è¯•è¯»å–ä¸€å¸§å¹¶è¯†åˆ«ã€‚
        è¿”å›: matched_user_id (int) æˆ– None
        """
        # å¦‚æœæ²¡æ‘„åƒå¤´æˆ–æ²¡äººè„¸åº“ï¼Œç›´æ¥è·³è¿‡
        if not self.cap or not self.known_face_encodings:
            return None

        # é¢‘ç‡æ§åˆ¶
        if time.time() - self.last_scan_time < self.scan_interval:
            return None
        self.last_scan_time = time.time()

        ret, frame = self.cap.read()
        if not ret:
            print("âš ï¸ [Face] æ— æ³•è¯»å–è§†é¢‘å¸§ (Stream broken)")
            # å°è¯•é‡è¿é€»è¾‘å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            return None

        # ä¼˜åŒ–ç­–ç•¥: å¼ºåˆ¶å›¾åƒå¢å¼º
        # 1. è½¬ä¸º LAB é¢œè‰²ç©ºé—´ (L=äº®åº¦, A/B=é¢œè‰²)
        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)

        # 2. å¯¹ L (äº®åº¦) é€šé“åº”ç”¨ CLAHE
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        cl = clahe.apply(l)

        # 3. åˆå¹¶é€šé“å¹¶è½¬å› RGB
        limg = cv2.merge((cl, a, b))
        enhanced_frame = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)

        # 3. æ£€æµ‹äººè„¸ (ä½¿ç”¨å¢å¼ºåçš„å›¾åƒ)
        face_locations = face_recognition.face_locations(enhanced_frame)
        
        if not face_locations:
            self.no_face_count += 1
            # è¿ç»­æœªæ£€æµ‹åˆ°äººè„¸æ—¶ï¼Œä»…åœ¨æ§åˆ¶å°æç¤ºï¼ˆä¸å†ä¿å­˜å›¾ç‰‡ï¼‰
            if self.no_face_count % 20 == 0: # é™ä½é¢‘ç‡åˆ°çº¦ 10ç§’ä¸€æ¬¡
                # print(f"âš ï¸ [Face] è¿ç»­ {self.no_face_count} æ¬¡æœªæ£€æµ‹åˆ°äººè„¸ (è¯·æ£€æŸ¥å…‰çº¿æˆ–è·ç¦»)")
                pass
            return None 
        
        # å¦‚æœæ£€æµ‹åˆ°äº†ï¼Œé‡ç½®è®¡æ•°å™¨
        if self.no_face_count > 0:
            # print("âœ¨ [Face] é‡æ–°æ•æ‰åˆ°äººè„¸")
            self.no_face_count = 0

        # 4. æå–ç‰¹å¾
        # æ³¨æ„ï¼šè™½ç„¶ç”¨å¢å¼ºå›¾æ£€æµ‹åˆ°äº†ä½ç½®ï¼Œä½†ä¸ºäº†ç‰¹å¾å‡†ç¡®æ€§ï¼Œå»ºè®®ï¼š
        # æ–¹æ¡ˆA: ç”¨å¢å¼ºå›¾æå–ç‰¹å¾ (æš—å…‰ä¸‹å¯èƒ½æ›´å¥½)
        # æ–¹æ¡ˆB: ç”¨åŸå›¾æå–ç‰¹å¾ (é¢œè‰²æ›´è‡ªç„¶)
        # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©æ–¹æ¡ˆ Aï¼Œå› ä¸ºåŸå›¾å¯èƒ½å¤ªæš—æ ¹æœ¬æä¸å‡ºç‰¹å¾
        face_encodings = face_recognition.face_encodings(enhanced_frame, face_locations)
        
        print(f"ğŸ‘€ [Face] æ•è·åˆ° {len(face_encodings)} å¼ äººè„¸")

        # 4. æ¯”å¯¹
        for face_encoding in face_encodings:
            # è®¡ç®—ä¸æ•°æ®åº“ä¸­æ‰€æœ‰äººè„¸çš„æ¬§æ°è·ç¦»
            # è·ç¦»è¶Šå°è¶Šç›¸ä¼¼ã€‚é€šå¸¸ 0.6 æ˜¯åˆ†ç•Œçº¿ã€‚
            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)
            
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„é‚£ä¸ª
            best_match_index = np.argmin(face_distances)
            min_distance = face_distances[best_match_index]

            # é˜ˆå€¼è°ƒæ•´è¯´æ˜:
            # 0.60: æ ‡å‡†ä¸¥æ ¼é˜ˆå€¼ (è¯¯è¯†ç‡æä½ï¼Œä½†æ‹’è¯†ç‡é«˜)
            # 0.72: å®½æ¾é˜ˆå€¼ (é€‚åˆæ ‘è“æ´¾æ‘„åƒå¤´åŠéå—æ§å…‰çº¿ï¼Œä½“éªŒæ›´å¥½)
            if min_distance < 0.72: 
                user_id = self.known_face_ids[best_match_index]
                print(f"ğŸ‘¤ [Face] è¯†åˆ«æˆåŠŸ! ID: {user_id} (è·ç¦»: {min_distance:.2f})")
                return user_id
            else:
                print(f"ğŸ¤” [Face] é™Œç”Ÿäºº (æœ€è¿‘è·ç¦»: {min_distance:.2f})")
        
        return None

    def close(self):
        if self.cap:
            self.cap.release()
